<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Friendly artificial intelligence - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Friendly_artificial_intelligence","wgTitle":"Friendly artificial intelligence","wgCurRevisionId":820112549,"wgRevisionId":820112549,"wgArticleId":351887,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["All articles with unsourced statements","Articles with unsourced statements from September 2014","Futurology","Philosophy of artificial intelligence","Singularitarianism","Transhumanism"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Friendly_artificial_intelligence","wgRelevantArticleId":351887,"wgRequestId":"Wn@oqApAAEMAAJ6BtwgAAADU","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":[],"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFExpandAllSectionsUserOption":false,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q4168796","wgScoreNoteLanguages":{"arabic":"العربية","catalan":"català","deutsch":"Deutsch","english":"English","espanol":"español","italiano":"italiano","nederlands":"Nederlands","norsk":"norsk","portugues":"português","suomi":"suomi","svenska":"svenska","vlaams":"West-Vlams"},"wgScoreDefaultNoteLanguage":"nederlands","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":false,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":true});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function($,jQuery,require,module){/*@nomin*/mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});
});mw.loader.load(["ext.cite.a11y","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.31.0-wmf.20"/>
<meta name="referrer" content="origin"/>
<meta name="referrer" content="origin-when-crossorigin"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Friendly_artificial_intelligence"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Friendly_artificial_intelligence"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Friendly_artificial_intelligence rootpage-Friendly_artificial_intelligence skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>
			<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div><div class="mw-indicators mw-body-content">
</div>
<h1 id="firstHeading" class="firstHeading" lang="en">Friendly artificial intelligence</h1>			<div id="bodyContent" class="mw-body-content">
				<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>				<div id="contentSub"></div>
								<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><p>A <b>friendly artificial intelligence</b> (also <b>friendly AI</b> or <b>FAI</b>) is a hypothetical <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> (AGI) that would have a positive effect on humanity. It is a part of the <a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">ethics of artificial intelligence</a> and is closely related to <a href="/wiki/Machine_ethics" title="Machine ethics">machine ethics</a>. While machine ethics is concerned with how an artificially intelligent agent <b>should</b> behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.</p>
<p></p>
<div id="toc" class="toc">
<div class="toctitle" lang="en" dir="ltr" xml:lang="en">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Etymology_and_usage"><span class="tocnumber">1</span> <span class="toctext">Etymology and usage</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Risks_of_unfriendly_AI"><span class="tocnumber">2</span> <span class="toctext">Risks of unfriendly AI</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Coherent_extrapolated_volition"><span class="tocnumber">3</span> <span class="toctext">Coherent extrapolated volition</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Other_approaches"><span class="tocnumber">4</span> <span class="toctext">Other approaches</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Public_policy"><span class="tocnumber">5</span> <span class="toctext">Public policy</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Criticism"><span class="tocnumber">6</span> <span class="toctext">Criticism</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#See_also"><span class="tocnumber">7</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#References"><span class="tocnumber">8</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Further_reading"><span class="tocnumber">9</span> <span class="toctext">Further reading</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#External_links"><span class="tocnumber">10</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Etymology_and_usage">Etymology and usage</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=1" title="Edit section: Etymology and usage">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The term was coined by <a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a>,<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup> who is best known for popularizing the idea,<sup id="cite_ref-aima_2-0" class="reference"><a href="#cite_note-aima-2">[2]</a></sup><sup id="cite_ref-3" class="reference"><a href="#cite_note-3">[3]</a></sup> to discuss <a href="/wiki/Superintelligence" title="Superintelligence">superintelligent</a> artificial agents that reliably implement human values. <a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a> and <a href="/wiki/Peter_Norvig" title="Peter Norvig">Peter Norvig</a>'s leading <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a> textbook, <i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i>, describes the idea:<sup id="cite_ref-4" class="reference"><a href="#cite_note-4">[4]</a></sup></p>
<blockquote>
<p>Yudkowsky (2008) goes into more detail about how to design a <b>Friendly AI</b>. He asserts that friendliness (a desire not to harm humans) should be designed in from the start, but that the designers should recognize both that their own designs may be flawed, and that the robot will learn and evolve over time. Thus the challenge is one of mechanism design—to define a mechanism for evolving AI systems under a system of checks and balances, and to give the systems utility functions that will remain friendly in the face of such changes.</p>
</blockquote>
<p>'Friendly' is used in this context as <a href="/wiki/Technical_terminology" class="mw-redirect" title="Technical terminology">technical terminology</a>, and picks out agents that are safe and useful, not necessarily ones that are "friendly" in the colloquial sense. The concept is primarily invoked in the context of discussions of recursively self-improving artificial agents that rapidly <a href="/wiki/Intelligence_explosion" title="Intelligence explosion">explode in intelligence</a>, on the grounds that this hypothetical technology would have a large, rapid, and difficult-to-control impact on human society.<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup></p>
<h2><span class="mw-headline" id="Risks_of_unfriendly_AI">Risks of unfriendly AI</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=2" title="Edit section: Risks of unfriendly AI">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></div>
<p>The roots of concern about artificial intelligence are very old. <a href="/w/index.php?title=Kevin_LaGrandeur&amp;action=edit&amp;redlink=1" class="new" title="Kevin LaGrandeur (page does not exist)">Kevin LaGrandeur</a> showed that the dangers specific to AI can be seen in ancient literature concerning artificial humanoid servants such as the <a href="/wiki/Golem" title="Golem">golem</a>, or the proto-robots of <a href="/wiki/Gerbert_of_Aurillac" class="mw-redirect" title="Gerbert of Aurillac">Gerbert of Aurillac</a> and <a href="/wiki/Roger_Bacon" title="Roger Bacon">Roger Bacon</a>. In those stories, the extreme intelligence and power of these humanoid creations clash with their status as slaves (which by nature are seen as sub-human), and cause disastrous conflict.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup> By 1942 these themes prompted <a href="/wiki/Isaac_Asimov" title="Isaac Asimov">Isaac Asimov</a> to create the "<a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws of Robotics</a>" - principles hard-wired into all the robots in his fiction, intended to prevent them from turning on their creators, or allow them to come to harm.<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup></p>
<p>In modern times as the prospect of <a href="/wiki/Superintelligence" title="Superintelligence">superintelligent AI</a> looms nearer, philosopher <a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a> has said that superintelligent AI systems with goals that are not aligned with human ethics are intrinsically dangerous unless extreme measures are taken to ensure the safety of humanity. He put it this way:</p>
<blockquote>
<p>Basically we should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.'</p>
</blockquote>
<p>Ryszard Michalski, a pioneer of <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, taught his Ph.D. students decades ago that any truly alien mind, including a machine mind, was unknowable and therefore dangerous to humans.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2014)">citation needed</span></a></i>]</sup></p>
<p>More recently, Eliezer Yudkowsky has called for the creation of “friendly AI” to mitigate <a href="/wiki/Existential_risk_from_advanced_artificial_intelligence" class="mw-redirect" title="Existential risk from advanced artificial intelligence">existential risk from advanced artificial intelligence</a>. He explains: "The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else."<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup></p>
<p><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a> says that a sufficiently advanced AI system will, unless explicitly counteracted, exhibit a number of <a href="/wiki/Instrumental_convergence#Basic_AI_drives" title="Instrumental convergence">basic "drives"</a>, such as resource acquisition, self preservation, and continuous self improvement, because of the intrinsic nature of any goal-driven systems and that these drives will, "without special precautions", cause the AI to exhibit undesired behavior.<sup id="cite_ref-9" class="reference"><a href="#cite_note-9">[9]</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup></p>
<p><a href="/wiki/Alexander_Wissner-Gross" title="Alexander Wissner-Gross">Alexander Wissner-Gross</a> says that AIs driven to maximize their future freedom of action (or causal path entropy) might be considered friendly if their planning horizon is longer than a certain threshold, and unfriendly if their planning horizon is shorter than that threshold.<sup id="cite_ref-11" class="reference"><a href="#cite_note-11">[11]</a></sup><sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup></p>
<p>Luke Muehlhauser, writing for the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a>, recommends that <a href="/wiki/Machine_ethics" title="Machine ethics">machine ethics</a> researchers adopt what <a href="/wiki/Bruce_Schneier" title="Bruce Schneier">Bruce Schneier</a> has called the "security mindset": Rather than thinking about how a system will work, imagine how it could fail. For instance, he suggests even an AI that only makes accurate predictions and communicates via a text interface might cause unintended harm.<sup id="cite_ref-MuehlhauserSecurity2013_13-0" class="reference"><a href="#cite_note-MuehlhauserSecurity2013-13">[13]</a></sup></p>
<h2><span class="mw-headline" id="Coherent_extrapolated_volition">Coherent extrapolated volition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=3" title="Edit section: Coherent extrapolated volition">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Yudkowsky advances the Coherent Extrapolated Volition (CEV) model. According to him, coherent extrapolated volition is people's choices and the actions people would collectively take if "we knew more, thought faster, were more the people we wished we were, and had grown up closer together."<sup id="cite_ref-cevpaper_14-0" class="reference"><a href="#cite_note-cevpaper-14">[14]</a></sup></p>
<p>Rather than a Friendly AI being designed directly by human programmers, it is to be designed by a "seed AI" programmed to first study <a href="/wiki/Human_nature" title="Human nature">human nature</a> and then produce the AI which humanity would want, given sufficient time and insight, to arrive at a satisfactory answer.<sup id="cite_ref-cevpaper_14-1" class="reference"><a href="#cite_note-cevpaper-14">[14]</a></sup> The appeal to an <a href="/wiki/Evolutionary_psychology" title="Evolutionary psychology">objective though contingent human nature</a> (perhaps expressed, for mathematical purposes, in the form of a <a href="/wiki/Utility_function" class="mw-redirect" title="Utility function">utility function</a> or other <a href="/wiki/Decision_theory" title="Decision theory">decision-theoretic</a> formalism), as providing the ultimate criterion of "Friendliness", is an answer to the <a href="/wiki/Metaethics" class="mw-redirect" title="Metaethics">meta-ethical</a> problem of defining an <a href="/wiki/Moral_universalism" title="Moral universalism">objective morality</a>; extrapolated volition is intended to be what humanity objectively would want, all things considered, but it can only be defined relative to the psychological and cognitive qualities of present-day, unextrapolated humanity.</p>
<h2><span class="mw-headline" id="Other_approaches">Other approaches</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=4" title="Edit section: Other approaches">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/Ben_Goertzel" title="Ben Goertzel">Ben Goertzel</a>, an <a href="/wiki/Artificial_general_intelligence" title="Artificial general intelligence">artificial general intelligence</a> researcher, believes that friendly AI cannot be created with current human knowledge. Goertzel suggests humans may instead decide to create an "AI Nanny" with "mildly superhuman intelligence and surveillance powers", to protect the human race from <a href="/wiki/Existential_risks" class="mw-redirect" title="Existential risks">existential risks</a> like <a href="/wiki/Nanotechnology" title="Nanotechnology">nanotechnology</a> and to delay the development of other (unfriendly) artificial intelligences until and unless the safety issues are solved.<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup> This can also be termed "Defensive AI."</p>
<p><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a> has proposed a "scaffolding" approach to AI safety, in which one provably safe AI generation helps build the next provably safe generation.<sup id="cite_ref-Hendry2014_16-0" class="reference"><a href="#cite_note-Hendry2014-16">[16]</a></sup></p>
<p>Stefan Pernar argues along the lines of <a href="/wiki/Meno#Meno.27s_paradox" title="Meno">Meno's paradox</a> to point out that attempting to solve the FAI problem is either pointless or hopeless depending on whether one assumes a universe that exhibits <a href="/wiki/Moral_realism" title="Moral realism">moral realism</a> or not. In the former case a transhuman AI would independently reason itself into the proper goal system and assuming the latter, designing a friendly AI would be futile to begin with since morals can not be reasoned about.<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup></p>
<p>Cindy Mason, an <a href="/wiki/AI" class="mw-redirect" title="AI">AI</a> researcher who has also worked with mind-body medicine at <a href="/wiki/Stanford_University_Medical_Center" title="Stanford University Medical Center">Stanford University Medical Center</a>, believes <a href="/wiki/Neuroplasticity" title="Neuroplasticity">neuroplasticity</a> and new discoveries of the hormone <a href="/wiki/Oxytocin" title="Oxytocin">oxytocin</a> mean compassionate intelligence is essential in AI systems that exhibit socially positive behaviors. She has proposed a set of software engineering principles for engineering kindness that includes a pro-human stance and an architecture for giving robots compassion.<sup id="cite_ref-18" class="reference"><a href="#cite_note-18">[18]</a></sup></p>
<h2><span class="mw-headline" id="Public_policy">Public policy</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=5" title="Edit section: Public policy">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><a href="/wiki/James_Barrat" title="James Barrat">James Barrat</a>, author of <i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i>, suggested that "a public-private partnership has to be created to bring A.I.-makers together to share ideas about security—something like the International Atomic Energy Agency, but in partnership with corporations." He urges AI researchers to convene a meeting similar to the <a href="/wiki/Asilomar_Conference_on_Recombinant_DNA" title="Asilomar Conference on Recombinant DNA">Asilomar Conference on Recombinant DNA</a>, which discussed risks of biotechnology.<sup id="cite_ref-Hendry2014_16-1" class="reference"><a href="#cite_note-Hendry2014-16">[16]</a></sup></p>
<p><a href="/wiki/John_McGinnis" title="John McGinnis">John McGinnis</a> encourages governments to accelerate friendly AI research. Because the goalposts of friendly AI aren't necessarily clear, he suggests a model more like the <a href="/wiki/National_Institutes_of_Health" title="National Institutes of Health">National Institutes of Health</a>, where "Peer review panels of computer and cognitive scientists would sift through projects and choose those that are designed both to advance AI and assure that such advances would be accompanied by appropriate safeguards." McGinnis feels that peer review is better "than regulation to address technical issues that are not possible to capture through bureaucratic mandates". McGinnis notes that his proposal stands in contrast to that of the <a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a>, which generally aims to avoid government involvement in friendly AI.<sup id="cite_ref-McGinnis2010_19-0" class="reference"><a href="#cite_note-McGinnis2010-19">[19]</a></sup></p>
<p>According to <a href="/wiki/Gary_Marcus" title="Gary Marcus">Gary Marcus</a>, the annual amount of money being spent on developing machine morality is tiny.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup></p>
<h2><span class="mw-headline" id="Criticism">Criticism</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=6" title="Edit section: Criticism">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">See also: <a href="/wiki/Technological_singularity#Criticisms" title="Technological singularity">Technological singularity §&#160;Criticisms</a></div>
<p>Some critics believe that both human-level AI and superintelligence are unlikely, and that therefore friendly AI is unlikely. Writing in <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i>, Alan Winfeld compares human-level artificial intelligence with faster-than-light travel in terms of difficulty, and states that while we need to be "cautious and prepared" given the stakes involved, we "don't need to be obsessing" about the risks of superintelligence.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup></p>
<p>Some philosophers claim that any truly "rational" agent, whether artificial or human, will naturally be benevolent; in this view, deliberate safeguards designed to produce a friendly AI could be unnecessary or even harmful.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup> Other critics question whether it is possible for an artificial intelligence to be friendly. Adam Keiper and Ari N. Schulman, editors of the technology journal <i><a href="/wiki/The_New_Atlantis_(journal)" title="The New Atlantis (journal)">The New Atlantis</a></i>, say that it will be impossible to ever guarantee "friendly" behavior in AIs because problems of ethical complexity will not yield to software advances or increases in computing power. They write that the criteria upon which friendly AI theories are based work "only when one has not only great powers of prediction about the likelihood of myriad possible outcomes, but certainty and consensus on how one values the different outcomes.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup></p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=7" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a href="/wiki/Intelligence_explosion" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
<li><a href="/wiki/Singularitarianism" title="Singularitarianism">Singularitarianism</a> – a moral philosophy advocated by proponents of Friendly AI</li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
<li><a href="/wiki/Three_Laws_of_Robotics" title="Three Laws of Robotics">Three Laws of Robotics</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=8" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation book">Tegmark, Max (2014). "Life, Our Universe and Everything". <i><a href="/wiki/Our_Mathematical_Universe:_My_Quest_for_the_Ultimate_Nature_of_Reality" class="mw-redirect" title="Our Mathematical Universe: My Quest for the Ultimate Nature of Reality">Our Mathematical Universe: My Quest for the Ultimate Nature of Reality</a></i> (First ed.). <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780307744258" title="Special:BookSources/9780307744258">9780307744258</a>. <q>Its owner may cede control to what Eliezer Yudkowsky terms a "Friendly AI,"...</q></cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Life%2C+Our+Universe+and+Everything&amp;rft.btitle=Our+Mathematical+Universe%3A+My+Quest+for+the+Ultimate+Nature+of+Reality&amp;rft.edition=First&amp;rft.date=2014&amp;rft.isbn=9780307744258&amp;rft.aulast=Tegmark&amp;rft.aufirst=Max&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-aima-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-aima_2-0">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Russell, Stuart</a>; <a href="/wiki/Peter_Norvig" title="Peter Norvig">Norvig, Peter</a> (2009). <i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i>. Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-13-604259-4" title="Special:BookSources/978-0-13-604259-4">978-0-13-604259-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pub=Prentice+Hall&amp;rft.date=2009&amp;rft.isbn=978-0-13-604259-4&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation book">Leighton, Jonathan (2011). <i>The Battle for Compassion: Ethics in an Apathetic Universe</i>. Algora. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-87586-870-7" title="Special:BookSources/978-0-87586-870-7">978-0-87586-870-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Battle+for+Compassion%3A+Ethics+in+an+Apathetic+Universe&amp;rft.pub=Algora&amp;rft.date=2011&amp;rft.isbn=978-0-87586-870-7&amp;rft.aulast=Leighton&amp;rft.aufirst=Jonathan&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-4">^</a></b></span> <span class="reference-text"><cite class="citation book">Russell, Stuart; Norvig, Peter (2010). <i><a href="/wiki/Artificial_Intelligence:_A_Modern_Approach" title="Artificial Intelligence: A Modern Approach">Artificial Intelligence: A Modern Approach</a></i>. Prentice Hall. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-13-604259-7" title="Special:BookSources/0-13-604259-7">0-13-604259-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Artificial+Intelligence%3A+A+Modern+Approach&amp;rft.pub=Prentice+Hall&amp;rft.date=2010&amp;rft.isbn=0-13-604259-7&amp;rft.aulast=Russell&amp;rft.aufirst=Stuart&amp;rft.au=Norvig%2C+Peter&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation book">Wallach, Wendell; Allen, Colin (2009). <i>Moral Machines: Teaching Robots Right from Wrong</i>. Oxford University Press, Inc. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-0-19-537404-9" title="Special:BookSources/978-0-19-537404-9">978-0-19-537404-9</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Moral+Machines%3A+Teaching+Robots+Right+from+Wrong&amp;rft.pub=Oxford+University+Press%2C+Inc.&amp;rft.date=2009&amp;rft.isbn=978-0-19-537404-9&amp;rft.aulast=Wallach&amp;rft.aufirst=Wendell&amp;rft.au=Allen%2C+Colin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation web"><a href="/w/index.php?title=Kevin_LaGrandeur&amp;action=edit&amp;redlink=1" class="new" title="Kevin LaGrandeur (page does not exist)">Kevin LaGrandeur</a>. <a rel="nofollow" class="external text" href="https://www.academia.edu/704751/_The_Persistent_Peril_of_the_Artificial_Slave_/">"The Persistent Peril of the Artificial Slave"</a>. Science Fiction Studies<span class="reference-accessdate">. Retrieved <span class="nowrap">2013-05-06</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Persistent+Peril+of+the+Artificial+Slave&amp;rft.pub=Science+Fiction+Studies&amp;rft.au=Kevin+LaGrandeur&amp;rft_id=https%3A%2F%2Fwww.academia.edu%2F704751%2F_The_Persistent_Peril_of_the_Artificial_Slave_%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite class="citation book">Isaac Asimov (1964). "Introduction". <i>The Rest of the Robots</i>. Doubleday. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/0-385-09041-2" title="Special:BookSources/0-385-09041-2">0-385-09041-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Introduction&amp;rft.btitle=The+Rest+of+the+Robots&amp;rft.pub=Doubleday&amp;rft.date=1964&amp;rft.isbn=0-385-09041-2&amp;rft.au=Isaac+Asimov&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a> (2008) in <i><a rel="nofollow" class="external text" href="http://intelligence.org/files/AIPosNegFactor.pdf">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></i></span></li>
<li id="cite_note-9"><span class="mw-cite-backlink"><b><a href="#cite_ref-9">^</a></b></span> <span class="reference-text">Omohundro, S. M. (2008, February). The basic AI drives. In AGI (Vol. 171, pp. 483-492).</span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation book">Bostrom, Nick (2014). <i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i>. Oxford: Oxford University Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780199678112" title="Special:BookSources/9780199678112">9780199678112</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.place=Oxford&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.isbn=9780199678112&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span> Chapter 7: The Superintelligent Will.</span></li>
<li id="cite_note-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-11">^</a></b></span> <span class="reference-text">'<a rel="nofollow" class="external text" href="http://io9.com/how-skynet-might-emerge-from-simple-physics-482402911">How Skynet Might Emerge From Simple Physics</a><i>, io9, Published 2013-04-26.</i></span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="/wiki/Alexander_Wissner-Gross" title="Alexander Wissner-Gross">Wissner-Gross, A. D.</a>; <a href="/w/index.php?title=Cameron_Freer&amp;action=edit&amp;redlink=1" class="new" title="Cameron Freer (page does not exist)">Freer, C. E.</a> (2013). <a rel="nofollow" class="external text" href="http://www.alexwg.org/link?url=http%3A%2F%2Fwww.alexwg.org%2Fpublications%2FPhysRevLett_110-168702.pdf">"Causal entropic forces"</a> <span style="font-size:85%;">(PDF)</span>. <i>Physical Review Letters</i>. <b>110</b> (16): 168702. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2013PhRvL.110p8702W">2013PhRvL.110p8702W</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1103%2FPhysRevLett.110.168702">10.1103/PhysRevLett.110.168702</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Physical+Review+Letters&amp;rft.atitle=Causal+entropic+forces&amp;rft.volume=110&amp;rft.issue=16&amp;rft.pages=168702&amp;rft.date=2013&amp;rft_id=info%3Adoi%2F10.1103%2FPhysRevLett.110.168702&amp;rft_id=info%3Abibcode%2F2013PhRvL.110p8702W&amp;rft.aulast=Wissner-Gross&amp;rft.aufirst=A.+D.&amp;rft.au=Freer%2C+C.+E.&amp;rft_id=http%3A%2F%2Fwww.alexwg.org%2Flink%3Furl%3Dhttp%253A%252F%252Fwww.alexwg.org%252Fpublications%252FPhysRevLett_110-168702.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-MuehlhauserSecurity2013-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-MuehlhauserSecurity2013_13-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Muehlhauser, Luke (31 Jul 2013). <a rel="nofollow" class="external text" href="http://intelligence.org/2013/07/31/ai-risk-and-the-security-mindset/">"AI Risk and the Security Mindset"</a>. <i>Machine Intelligence Research Institute</i><span class="reference-accessdate">. Retrieved <span class="nowrap">15 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Machine+Intelligence+Research+Institute&amp;rft.atitle=AI+Risk+and+the+Security+Mindset&amp;rft.date=2013-07-31&amp;rft.aulast=Muehlhauser&amp;rft.aufirst=Luke&amp;rft_id=http%3A%2F%2Fintelligence.org%2F2013%2F07%2F31%2Fai-risk-and-the-security-mindset%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-cevpaper-14"><span class="mw-cite-backlink">^ <a href="#cite_ref-cevpaper_14-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-cevpaper_14-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://intelligence.org/files/CEV.pdf">"Coherent Extrapolated Volition"</a> <span style="font-size:85%;">(PDF)</span>. Intelligence.org<span class="reference-accessdate">. Retrieved <span class="nowrap">2015-09-12</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Coherent+Extrapolated+Volition&amp;rft.pub=Intelligence.org&amp;rft_id=https%3A%2F%2Fintelligence.org%2Ffiles%2FCEV.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Goertzel, Ben. "<a rel="nofollow" class="external text" href="https://web.archive.org/web/20140408142320/http://commonsenseatheism.com/wp-content/uploads/2012/03/Goertzel-Should-Humanity-Build-a-Global-AI-Nanny-to-Delay-the-Singularity-Until-its-Better-Understood.pdf">Should Humanity Build a Global AI Nanny to Delay the Singularity Until It’s Better Understood?</a>", Journal of consciousness studies 19.1-2 (2012): 1-2.</span></li>
<li id="cite_note-Hendry2014-16"><span class="mw-cite-backlink">^ <a href="#cite_ref-Hendry2014_16-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Hendry2014_16-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation news">Hendry, Erica R. (21 Jan 2014). <a rel="nofollow" class="external text" href="http://www.smithsonianmag.com/innovation/what-happens-when-artificial-intelligence-turns-us-180949415/">"What Happens When Artificial Intelligence Turns On Us?"</a>. Smithsonian.com<span class="reference-accessdate">. Retrieved <span class="nowrap">15 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=What+Happens+When+Artificial+Intelligence+Turns+On+Us%3F&amp;rft.date=2014-01-21&amp;rft.aulast=Hendry&amp;rft.aufirst=Erica+R.&amp;rft_id=http%3A%2F%2Fwww.smithsonianmag.com%2Finnovation%2Fwhat-happens-when-artificial-intelligence-turns-us-180949415%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">Pernar, Stefan. "<a rel="nofollow" class="external text" href="http://rationalmorality.info/wp-content/uploads/2015/04/TranshumanPhilosophy_formatted.pdf">The Evolutionary Perspective - a Transhuman Philosophy</a>", 8th Conference on Artificial General Intelligence in Berlin, July 22-25, 2015</span></li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Mason, Cindy. "<a rel="nofollow" class="external text" href="https://pdfs.semanticscholar.org/02da/b0b1022c940788c016792ab3f08ad1d88cac.pdf">Engineering Kindness - Giving Machines Compassionate Intelligence</a>", International Journal of Synthetic Emotions, 6(1), June – December 2015.</span></li>
<li id="cite_note-McGinnis2010-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-McGinnis2010_19-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">McGinnis, John O. (Summer 2010). <a rel="nofollow" class="external text" href="http://www.law.northwestern.edu/LAWREVIEW/Colloquy/2010/12/">"Accelerating AI"</a>. <i>Northwestern University Law Review</i>. <b>104</b> (3): 1253–1270<span class="reference-accessdate">. Retrieved <span class="nowrap">16 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Northwestern+University+Law+Review&amp;rft.atitle=Accelerating+AI&amp;rft.ssn=spring&amp;rft.volume=104&amp;rft.issue=3&amp;rft.pages=1253-1270&amp;rft.date=2010&amp;rft.aulast=McGinnis&amp;rft.aufirst=John+O.&amp;rft_id=http%3A%2F%2Fwww.law.northwestern.edu%2FLAWREVIEW%2FColloquy%2F2010%2F12%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation news">Marcus, Gary (24 November 2012). <a rel="nofollow" class="external text" href="http://www.newyorker.com/news/news-desk/moral-machines">"Moral Machines"</a>. <a href="/wiki/The_New_Yorker" title="The New Yorker">The New Yorker</a><span class="reference-accessdate">. Retrieved <span class="nowrap">30 July</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Moral+Machines&amp;rft.date=2012-11-24&amp;rft.aulast=Marcus&amp;rft.aufirst=Gary&amp;rft_id=http%3A%2F%2Fwww.newyorker.com%2Fnews%2Fnews-desk%2Fmoral-machines&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation news">Winfield, Alan. <a rel="nofollow" class="external text" href="https://www.theguardian.com/technology/2014/aug/10/artificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield">"Artificial intelligence will not turn into a Frankenstein's monster"</a>. <i><a href="/wiki/The_Guardian" title="The Guardian">The Guardian</a></i><span class="reference-accessdate">. Retrieved <span class="nowrap">17 September</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Artificial+intelligence+will+not+turn+into+a+Frankenstein%27s+monster&amp;rft.aulast=Winfield&amp;rft.aufirst=Alan&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Ftechnology%2F2014%2Faug%2F10%2Fartificial-intelligence-will-not-become-a-frankensteins-monster-ian-winfield&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text">Kornai, András. "<a rel="nofollow" class="external text" href="http://www.kornai.com/Papers/agi12.pdf">Bounding the impact of AGI</a>". Journal of Experimental &amp; Theoretical Artificial Intelligence ahead-of-print (2014): 1-22. "...the essence of AGIs is their reasoning facilities, and it is the very logic of their being that will compel them to behave in a moral fashion... The real nightmare scenario (is one where) humans find it advantageous to strongly couple themselves to AGIs, with no guarantees against self-deception."</span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation web">Adam Keiper and Ari N. Schulman. <a rel="nofollow" class="external text" href="http://www.thenewatlantis.com/publications/the-problem-with-friendly-artificial-intelligence">"The Problem with 'Friendly' Artificial Intelligence"</a>. <i>The New Atlantis</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2012-01-16</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=The+Problem+with+%E2%80%98Friendly%E2%80%99+Artificial+Intelligence&amp;rft.pub=%27%27The+New+Atlantis%27%27&amp;rft.au=Adam+Keiper+and+Ari+N.+Schulman&amp;rft_id=http%3A%2F%2Fwww.thenewatlantis.com%2Fpublications%2Fthe-problem-with-friendly-artificial-intelligence&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AFriendly+artificial+intelligence" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
</ol>
</div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=9" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li>Yudkowsky, E. <a rel="nofollow" class="external text" href="http://intelligence.org/files/AIPosNegFactor.pdf">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a>. In <i>Global Catastrophic Risks</i>, Oxford University Press, 2008.<br />
Discusses Artificial Intelligence from the perspective of <a href="/wiki/Existential_risk" class="mw-redirect" title="Existential risk">Existential risk</a>, introducing the term "Friendly AI". In particular, Sections 1-4 give background to the definition of Friendly AI in Section 5. Section 6 gives two classes of mistakes (technical and philosophical) which would both lead to the accidental creation of non-Friendly AIs. Sections 7-13 discuss further related issues.</li>
<li>Omohundro, S. 2008 <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.393.8356&amp;rep=rep1&amp;type=pdf">The Basic AI Drives</a> Appeared in AGI-08 - Proceedings of the First Conference on Artificial General Intelligence</li>
<li>Mason, C. 2008 <a rel="nofollow" class="external text" href="https://aaai.org/Papers/Workshops/2008/WS-08-07/WS08-07-023.pdf">Human-Level AI Requires Compassionate Intelligence</a> Appears in <a href="/wiki/AAAI" class="mw-redirect" title="AAAI">AAAI</a> 2008 Workshop on Meta-Reasoning:Thinking About Thinking</li>
</ul>
<p><br /></p>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit&amp;section=10" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="http://www.nickbostrom.com/ethics/ai.html">Ethical Issues in Advanced Artificial Intelligence</a> by Nick Bostrom</li>
<li><a rel="nofollow" class="external text" href="https://intelligence.org/ie-faq/#WhatIsFriendlyAI">What is Friendly AI?</a> — A brief description of Friendly AI by the Machine Intelligence Research Institute.</li>
<li><a rel="nofollow" class="external text" href="https://intelligence.org/files/CFAI.pdf">Creating Friendly AI 1.0: The Analysis and Design of Benevolent Goal Architectures</a> — A near book-length description from the MIRI</li>
<li><a rel="nofollow" class="external text" href="http://www.ssec.wisc.edu/~billh/g/SIAI_critique.html">Critique of the MIRI Guidelines on Friendly AI</a> — by <a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a rel="nofollow" class="external text" href="http://www.optimal.org/peter/siai_guidelines.htm">Commentary on MIRI's Guidelines on Friendly AI</a> — by Peter Voss.</li>
<li><a rel="nofollow" class="external text" href="http://www.thenewatlantis.com/publications/the-problem-with-friendly-artificial-intelligence">The Problem with ‘Friendly’ Artificial Intelligence</a> — On the motives for and impossibility of FAI; by Adam Keiper and Ari N. Schulman.</li>
</ul>
<div role="navigation" class="navbox" aria-labelledby="Risks_from_artificial_intelligence" style="padding:3px">
<table class="nowraplinks collapsible collapsed navbox-inner" style="border-spacing:0;background:transparent;color:inherit">
<tr>
<th scope="col" class="navbox-title" colspan="2">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="/wiki/Template:Existential_risk_from_artificial_intelligence" title="Template:Existential risk from artificial intelligence"><abbr title="View this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none;">v</abbr></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Existential_risk_from_artificial_intelligence" title="Template talk:Existential risk from artificial intelligence"><abbr title="Discuss this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none;">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit"><abbr title="Edit this template" style=";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none;">e</abbr></a></li>
</ul>
</div>
<div id="Risks_from_artificial_intelligence" style="font-size:114%;margin:0 4em">Risks from <a href="/wiki/Artificial_intelligence" title="Artificial intelligence">artificial intelligence</a></div>
</th>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:1%">Concepts</th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="/wiki/AI_box" title="AI box">AI box</a></li>
<li><a href="/wiki/AI_takeover" title="AI takeover">AI takeover</a></li>
<li><a href="/wiki/AI_control_problem" title="AI control problem">Control problem</a></li>
<li><a href="/wiki/Existential_risk_from_artificial_general_intelligence" title="Existential risk from artificial general intelligence">Existential risk from artificial general intelligence</a></li>
<li><a class="mw-selflink selflink">Friendly artificial intelligence</a></li>
<li><a href="/wiki/Instrumental_convergence" title="Instrumental convergence">Instrumental convergence</a></li>
<li><a href="/wiki/Intelligence_explosion" title="Intelligence explosion">Intelligence explosion</a></li>
<li><a href="/wiki/Machine_ethics" title="Machine ethics">Machine ethics</a></li>
<li><a href="/wiki/Superintelligence" title="Superintelligence">Superintelligence</a></li>
<li><a href="/wiki/Technological_singularity" title="Technological singularity">Technological singularity</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:1%">Organizations</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="/wiki/Allen_Institute_for_Artificial_Intelligence" title="Allen Institute for Artificial Intelligence">Allen Institute for Artificial Intelligence</a></li>
<li><a href="/wiki/Center_for_Applied_Rationality" title="Center for Applied Rationality">Center for Applied Rationality</a></li>
<li><a href="/wiki/Centre_for_the_Study_of_Existential_Risk" title="Centre for the Study of Existential Risk">Centre for the Study of Existential Risk</a></li>
<li><a href="/wiki/Foundational_Questions_Institute" title="Foundational Questions Institute">Foundational Questions Institute</a></li>
<li><a href="/wiki/Future_of_Humanity_Institute" title="Future of Humanity Institute">Future of Humanity Institute</a></li>
<li><a href="/wiki/Future_of_Life_Institute" title="Future of Life Institute">Future of Life Institute</a></li>
<li><a href="/wiki/Humanity%2B" title="Humanity+">Humanity+</a></li>
<li><a href="/wiki/Institute_for_Ethics_and_Emerging_Technologies" title="Institute for Ethics and Emerging Technologies">Institute for Ethics and Emerging Technologies</a></li>
<li><a href="/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence" title="Leverhulme Centre for the Future of Intelligence">Leverhulme Centre for the Future of Intelligence</a></li>
<li><a href="/wiki/Machine_Intelligence_Research_Institute" title="Machine Intelligence Research Institute">Machine Intelligence Research Institute</a></li>
<li><a href="/wiki/OpenAI" title="OpenAI">OpenAI</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:1%">People</th>
<td class="navbox-list navbox-odd hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="/wiki/Nick_Bostrom" title="Nick Bostrom">Nick Bostrom</a></li>
<li><a href="/wiki/Stephen_Hawking" title="Stephen Hawking">Stephen Hawking</a></li>
<li><a href="/wiki/Bill_Hibbard" title="Bill Hibbard">Bill Hibbard</a></li>
<li><a href="/wiki/Bill_Joy" title="Bill Joy">Bill Joy</a></li>
<li><a href="/wiki/Elon_Musk" title="Elon Musk">Elon Musk</a></li>
<li><a href="/wiki/Steve_Omohundro" title="Steve Omohundro">Steve Omohundro</a></li>
<li><a href="/wiki/Huw_Price" title="Huw Price">Huw Price</a></li>
<li><a href="/wiki/Martin_Rees" title="Martin Rees">Martin Rees</a></li>
<li><a href="/wiki/Stuart_J._Russell" title="Stuart J. Russell">Stuart J. Russell</a></li>
<li><a href="/wiki/Jaan_Tallinn" title="Jaan Tallinn">Jaan Tallinn</a></li>
<li><a href="/wiki/Max_Tegmark" title="Max Tegmark">Max Tegmark</a></li>
<li><a href="/wiki/Frank_Wilczek" title="Frank Wilczek">Frank Wilczek</a></li>
<li><a href="/wiki/Roman_Yampolskiy" title="Roman Yampolskiy">Roman Yampolskiy</a></li>
<li><a href="/wiki/Eliezer_Yudkowsky" title="Eliezer Yudkowsky">Eliezer Yudkowsky</a></li>
<li><a href="/wiki/Sam_Harris" title="Sam Harris">Sam Harris</a></li>
</ul>
</div>
</td>
</tr>
<tr>
<th scope="row" class="navbox-group" style="width:1%">Other</th>
<td class="navbox-list navbox-even hlist" style="text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px">
<div style="padding:0em 0.25em">
<ul>
<li><a href="/wiki/Open_Letter_on_Artificial_Intelligence" title="Open Letter on Artificial Intelligence">Open Letter on Artificial Intelligence</a></li>
<li><a href="/wiki/Ethics_of_artificial_intelligence" title="Ethics of artificial intelligence">Ethics of artificial intelligence</a></li>
<li><a href="/wiki/Artificial_general_intelligence#Controversies_and_dangers" title="Artificial general intelligence">Controversies and dangers of artificial general intelligence</a></li>
<li><a href="/wiki/Global_catastrophic_risk#Artificial_intelligence" title="Global catastrophic risk">Artificial intelligence as a global catastrophic risk</a></li>
<li><i><a href="/wiki/Superintelligence:_Paths,_Dangers,_Strategies" title="Superintelligence: Paths, Dangers, Strategies">Superintelligence: Paths, Dangers, Strategies</a></i></li>
<li><i><a href="/wiki/Our_Final_Invention" title="Our Final Invention">Our Final Invention</a></i></li>
</ul>
</div>
</td>
</tr>
</table>
</div>


<!-- 
NewPP limit report
Parsed by mw1264
Cached time: 20180211010507
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.196 seconds
Real time usage: 0.251 seconds
Preprocessor visited node count: 1032/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 38798/2097152 bytes
Template argument size: 630/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 1/500
Lua time usage: 0.098/10.000 seconds
Lua memory usage: 3.69 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  216.994      1 -total
 61.69%  133.857      1 Template:Reflist
 30.03%   65.157      7 Template:Cite_book
 16.88%   36.630      1 Template:Citation_needed
 14.92%   32.385      1 Template:Fix
 10.03%   21.755      2 Template:Category_handler
  9.71%   21.071      1 Template:Main
  6.58%   14.267      4 Template:Cite_web
  5.85%   12.689      3 Template:Cite_news
  4.72%   10.235      1 Template:Existential_risk_from_artificial_intelligence
-->
</div>
<!-- Saved in parser cache with key enwiki:pcache:idhash:351887-0!canonical and timestamp 20180211010507 and revision id 820112549
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=820112549">https://en.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=820112549</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Futurology" title="Category:Futurology">Futurology</a></li><li><a href="/wiki/Category:Philosophy_of_artificial_intelligence" title="Category:Philosophy of artificial intelligence">Philosophy of artificial intelligence</a></li><li><a href="/wiki/Category:Singularitarianism" title="Category:Singularitarianism">Singularitarianism</a></li><li><a href="/wiki/Category:Transhumanism" title="Category:Transhumanism">Transhumanism</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_September_2014" title="Category:Articles with unsourced statements from September 2014">Articles with unsourced statements from September 2014</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>
			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Friendly+artificial+intelligence" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Friendly+artificial+intelligence" title="You&#039;re encouraged to log in; however, it&#039;s not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
							<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Friendly_artificial_intelligence" title="View the content page [c]" accesskey="c">Article</a></span></li><li id="ca-talk"><span><a href="/wiki/Talk:Friendly_artificial_intelligence" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
							<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Friendly_artificial_intelligence">Read</a></span></li><li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li><li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span></h3>
						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>
						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
								<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>
			<div class="body">
								<ul>
					<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
			<h3 id='p-interaction-label'>Interaction</h3>
			<div class="body">
								<ul>
					<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>
			<div class="body">
								<ul>
					<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Friendly_artificial_intelligence" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Friendly_artificial_intelligence" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;oldid=820112549" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4168796" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Friendly_artificial_intelligence&amp;id=820112549" title="Information on how to cite this page">Cite this page</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
			<h3 id='p-coll-print_export-label'>Print/export</h3>
			<div class="body">
								<ul>
					<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Friendly+artificial+intelligence">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Friendly+artificial+intelligence&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Friendly_artificial_intelligence&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>				</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
			<h3 id='p-lang-label'>Languages</h3>
			<div class="body">
								<ul>
					<li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Inteligencia_artificial_amigable" title="Inteligencia artificial amigable – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Intelligence_artificielle_amicale" title="Intelligence artificielle amicale – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/Friendly_artificial_intelligence" title="Friendly artificial intelligence – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%94%D1%80%D1%83%D0%B6%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D1%82%D0%B5%D0%BB%D0%BB%D0%B5%D0%BA%D1%82" title="Дружественный искусственный интеллект – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-sv"><a href="https://sv.wikipedia.org/wiki/V%C3%A4nlig_artificiell_intelligens" title="Vänlig artificiell intelligens – Swedish" lang="sv" hreflang="sv" class="interlanguage-link-target">Svenska</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%94%D1%80%D1%83%D0%B6%D0%BD%D1%96%D0%B9_%D1%88%D1%82%D1%83%D1%87%D0%BD%D0%B8%D0%B9_%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82" title="Дружній штучний інтелект – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E5%8F%8B%E5%A5%BD%E7%9A%84%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD" title="友好的人工智能 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>				</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q4168796#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
				<div id="footer" role="contentinfo">
						<ul id="footer-info">
								<li id="footer-info-lastmod"> This page was last edited on 13 January 2018, at 03:33.</li>
								<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
							</ul>
						<ul id="footer-places">
								<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
								<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
								<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
								<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
								<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
								<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
								<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Friendly_artificial_intelligence&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
							</ul>
										<ul id="footer-icons" class="noprint">
										<li id="footer-copyrightico">
						<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>					</li>
										<li id="footer-poweredbyico">
						<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>					</li>
									</ul>
						<div style="clear: both;"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.196","walltime":"0.251","ppvisitednodes":{"value":1032,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":38798,"limit":2097152},"templateargumentsize":{"value":630,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":1,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  216.994      1 -total"," 61.69%  133.857      1 Template:Reflist"," 30.03%   65.157      7 Template:Cite_book"," 16.88%   36.630      1 Template:Citation_needed"," 14.92%   32.385      1 Template:Fix"," 10.03%   21.755      2 Template:Category_handler","  9.71%   21.071      1 Template:Main","  6.58%   14.267      4 Template:Cite_web","  5.85%   12.689      3 Template:Cite_news","  4.72%   10.235      1 Template:Existential_risk_from_artificial_intelligence"]},"scribunto":{"limitreport-timeusage":{"value":"0.098","limit":"10.000"},"limitreport-memusage":{"value":3870654,"limit":52428800}},"cachereport":{"origin":"mw1264","timestamp":"20180211010507","ttl":1900800,"transientcontent":false}}});});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":59,"wgHostname":"mw1272"});});</script>
	</body>
</html>
